{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Scraping clinical cases...\n",
      "Scraped 10 pages...\n",
      "Scraped 20 pages...\n",
      "Scraped 30 pages...\n",
      "Scraped 40 pages...\n",
      "Scraped 50 pages...\n",
      "Scraped 60 pages...\n",
      "Scraped 70 pages...\n",
      "Scraped 80 pages...\n",
      "Scraped 90 pages...\n",
      "Reached articles older than 2013. Exiting.\n",
      "Step 2: Extracting answers for each case...\n",
      "Extracted answers for 10 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 20 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 30 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 40 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 50 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 60 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 70 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 80 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 90 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 100 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 110 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 120 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 130 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 140 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 150 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 160 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 170 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 180 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 190 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 200 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 210 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 220 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 230 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 240 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 250 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 260 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 270 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 280 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 290 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 300 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 310 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 320 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 330 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 340 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 350 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 360 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 370 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 380 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 390 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 400 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 410 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 420 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 430 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 440 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 450 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 460 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 470 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 480 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 490 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 500 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 510 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 520 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 530 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 540 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 550 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 560 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 570 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 580 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 590 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 600 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 610 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 620 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 630 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 640 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 650 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 660 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 670 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 680 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 690 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 700 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 710 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 720 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 730 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 740 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 750 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 760 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 770 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 780 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 790 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 800 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 810 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 820 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 830 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 840 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 850 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 860 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 870 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 880 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 890 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 900 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 910 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 920 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 930 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 940 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 950 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 960 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 970 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 980 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 990 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1000 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1010 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1020 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1030 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1040 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1050 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1060 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1070 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1080 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1090 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1100 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1110 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1120 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1130 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1140 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1150 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1160 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1170 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1180 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1190 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1200 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1210 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1220 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1230 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1240 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1250 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1260 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1270 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1280 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1290 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1300 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1310 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1320 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1330 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1340 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1350 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1360 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1370 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1380 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1390 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1400 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1410 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1420 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1430 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1440 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1450 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1460 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1470 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1480 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1490 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1500 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1510 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1520 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1530 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1540 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1550 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1560 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1570 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1580 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1590 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1600 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1610 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1620 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1630 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1640 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1650 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1660 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1670 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1680 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1690 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1700 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1710 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1720 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1730 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1740 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1750 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1760 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1770 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1780 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1790 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1800 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1810 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1820 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Extracted answers for 1830 cases...\n",
      "Saved progress to JSON and CSV files.\n",
      "Step 3: Saving final results...\n",
      "Completed. Saved 1834 cases to JSON and CSV files.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# URL of the JAMA Network Clinical Challenges page\n",
    "BASE_URL = \"https://jamanetwork.com/collections/44038/clinical-challenge\"\n",
    "\n",
    "def scrape_clinical_cases():\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    cases_year = []\n",
    "    page_number = 1\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{BASE_URL}?page={page_number}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data from page {page_number}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        case_elements = soup.find_all(\"li\", class_=\"article\")\n",
    "        \n",
    "        if not case_elements:\n",
    "            print(\"No more cases found.\")\n",
    "            break\n",
    "        \n",
    "        for case in case_elements:\n",
    "            link_tag = case.find(\"a\", class_=\"article--title\")\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "\n",
    "            date_tag = case.find(\"div\", class_=\"article--date meta-item no-wrap\")\n",
    "            date_text = date_tag.text.strip() if date_tag else None\n",
    "            \n",
    "            if date_text:\n",
    "                publication_date = datetime.strptime(date_text, \"%B %d, %Y\")\n",
    "                if publication_date.year < 2013:\n",
    "                    print(\"Reached articles older than 2013. Exiting.\")\n",
    "                    return cases_year\n",
    "                \n",
    "                formatted_date = publication_date.strftime(\"%m_%Y\")\n",
    "                \n",
    "                if link:\n",
    "                    cases_year.append((link, formatted_date))\n",
    "\n",
    "        if page_number % 10 == 0:\n",
    "            print(f\"Scraped {page_number} pages...\")\n",
    "\n",
    "        page_number += 1\n",
    "    \n",
    "    return cases_year\n",
    "\n",
    "def extract_answers(case_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(case_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    diagnosis_section = soup.find(\"div\", class_=\"h4 cb section-type-section\")\n",
    "    diagnosis_title = diagnosis_section.find(\"p\", class_=\"para\").text.strip() if diagnosis_section else None\n",
    "    \n",
    "    answers = []\n",
    "    for answer in soup.find_all(\"p\", class_=\"para\"):\n",
    "        answer_text = answer.text.strip()\n",
    "        if answer_text.startswith(\"A.\") or answer_text.startswith(\"B.\") or answer_text.startswith(\"C.\") or answer_text.startswith(\"D.\"):\n",
    "            answers.append(answer_text)\n",
    "\n",
    "    answer_idx = answers[0][0] if answers else None\n",
    "    answer = answers[0][3:] if answers else None\n",
    "    \n",
    "    return answer_idx, answer\n",
    "\n",
    "def save_to_json(cases, filename='jama_clinical_cases.json'):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(cases, json_file, indent=4)\n",
    "\n",
    "def save_to_csv(cases, filename='jama_clinical_cases.csv'):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['id', 'link', 'publication_year', 'answer_idx', 'answer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for case in cases:\n",
    "            writer.writerow(case)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Step 1: Scraping clinical cases...\")\n",
    "    clinical_cases_links = scrape_clinical_cases()\n",
    "    \n",
    "    print(\"Step 2: Extracting answers for each case...\")\n",
    "    compiled_results = []\n",
    "    \n",
    "    for idx, (link, year) in enumerate(clinical_cases_links):\n",
    "        answer_idx, answer = extract_answers(link)\n",
    "        compiled_results.append({\n",
    "            'id': idx,\n",
    "            'link': link,\n",
    "            'publication_year': year,\n",
    "            'answer_idx': answer_idx,\n",
    "            'answer': answer\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Extracted answers for {idx + 1} cases...\")\n",
    "            save_to_json(compiled_results)\n",
    "            save_to_csv(compiled_results)\n",
    "            print(f\"Saved progress to JSON and CSV files.\")\n",
    "    \n",
    "    print(\"Step 3: Saving final results...\")\n",
    "    save_to_json(compiled_results)\n",
    "    save_to_csv(compiled_results)\n",
    "\n",
    "    print(f\"Completed. Saved {len(compiled_results)} cases to JSON and CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import pdb \n",
    "# URL of the JAMA Network Clinical Challenges page\n",
    "BASE_URL = \"https://jamanetwork.com/collections/44038/clinical-challenge\" # \n",
    "\n",
    "# Function to scrape the clinical cases\n",
    "\n",
    "def scrape_clinical_cases():\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    cases_year = []\n",
    "    page_number = 1\n",
    "    \n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{BASE_URL}?page={page_number}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data from page {page_number}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate each clinical case and extract the link\n",
    "        case_elements = soup.find_all(\"li\", class_=\"article\")  # General class to capture all JAMA medical field articles\n",
    "        \n",
    "        if not case_elements:\n",
    "            print(\"No more cases found.\")\n",
    "            break\n",
    "        \n",
    "        for case in case_elements:\n",
    "            link_tag = case.find(\"a\", class_=\"article--title\") \n",
    "            link = link_tag['href'] if link_tag else None\n",
    "\n",
    "            # Extract the publication date\n",
    "            date_tag = case.find(\"div\", class_=\"article--date meta-item no-wrap\")\n",
    "            date_text = date_tag.text.strip() if date_tag else None\n",
    "            \n",
    "            # Parse the date and check the year\n",
    "            if date_text:\n",
    "                publication_date = datetime.strptime(date_text, \"%B %d, %Y\")\n",
    "                # pdb.set_trace()\n",
    "                if publication_date.year < 2013:\n",
    "                    print(\"Reached articles older than 2013. Exiting.\")\n",
    "                    return cases_year  # Exit if the year is below 2013\n",
    "            \n",
    "            if link:\n",
    "                cases_year.append((link,publication_date.year))\n",
    "\n",
    "        print(f\"Case URLs of page {page_number} fetched...\")\n",
    "        page_number += 1  # Move to the next page\n",
    "\n",
    "        # if page_number > 2:\n",
    "        #     break # for debugging\n",
    "    return cases_year\n",
    "    \n",
    "# Function to extract answer_idx and answer from the clinical case page\n",
    "def extract_answers(case_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(case_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None, None  # Return None if there's an error\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Locate the section for the diagnosis\n",
    "    diagnosis_section = soup.find(\"div\", class_=\"h4 cb section-type-section\")\n",
    "    \n",
    "    # Extract the diagnosis title\n",
    "    diagnosis_title = diagnosis_section.find(\"p\", class_=\"para\").text.strip() if diagnosis_section else None\n",
    "    \n",
    "    # Now, find all the answers following the diagnosis section\n",
    "    answers = []\n",
    "    for answer in soup.find_all(\"p\", class_=\"para\"):\n",
    "        answer_text = answer.text.strip()\n",
    "        if answer_text.startswith(\"A.\") or answer_text.startswith(\"B.\") or answer_text.startswith(\"C.\") or answer_text.startswith(\"D.\"):\n",
    "            answers.append(answer_text)\n",
    "\n",
    "    # Assuming the first answer is the answer you want\n",
    "    answer_idx = answers[0][0] if answers else None  # Get the first character (A, B, C, or D)\n",
    "    answer = answers[0] if answers else None  # Get the full answer text\n",
    "    answer = answer[3:] # convert 'D. Sternoclavicular sinus' to 'Sternoclavicular sinus'\n",
    "    # pdb.set_trace()\n",
    "    return answer_idx, answer\n",
    "\n",
    "# Function to save the cases to a JSON file\n",
    "def save_to_json(cases, filename='jama_clinical_cases.json'):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(cases, json_file, indent=4)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    clinical_cases_links = scrape_clinical_cases()\n",
    "    \n",
    "    compiled_results = []\n",
    "    \n",
    "    for idx, (link, year) in enumerate(clinical_cases_links):\n",
    "        answer_idx, answer = extract_answers(link)\n",
    "        compiled_results.append({\n",
    "            'id': idx,\n",
    "            'link': link,\n",
    "            'publication_year': year,\n",
    "            'answer_idx': answer_idx,\n",
    "            'answer': answer\n",
    "        })\n",
    "    \n",
    "    # Save the results to a JSON file\n",
    "    with open('jama_clinical_cases.json', 'w') as json_file:\n",
    "        json.dump(compiled_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Saved {len(compiled_results)} cases to jama_clinical_cases.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Original with dates\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import pdb \n",
    "# URL of the JAMA Network Clinical Challenges page\n",
    "BASE_URL = \"https://jamanetwork.com/collections/44038/clinical-challenge\" # \n",
    "\n",
    "# Function to scrape the clinical cases\n",
    "\n",
    "def scrape_clinical_cases():\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    cases_year = []\n",
    "    page_number = 1\n",
    "    \n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{BASE_URL}?page={page_number}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data from page {page_number}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate each clinical case and extract the link\n",
    "        case_elements = soup.find_all(\"li\", class_=\"article\")  # General class to capture all JAMA medical field articles\n",
    "        \n",
    "        if not case_elements:\n",
    "            print(\"No more cases found.\")\n",
    "            break\n",
    "        \n",
    "        for case in case_elements:\n",
    "            link_tag = case.find(\"a\", class_=\"article--title\") \n",
    "            link = link_tag['href'] if link_tag else None\n",
    "\n",
    "            # Extract the publication date\n",
    "            date_tag = case.find(\"div\", class_=\"article--date meta-item no-wrap\")\n",
    "            date_text = date_tag.text.strip() if date_tag else None\n",
    "            \n",
    "            # Parse the date and check the year\n",
    "            if date_text:\n",
    "                publication_date = datetime.strptime(date_text, \"%B %d, %Y\")\n",
    "                if publication_date.year < 2013:\n",
    "                    print(\"Reached articles older than 2013. Exiting.\")\n",
    "                    break  # Exit if the year is below 2013\n",
    "                \n",
    "                # Format the month and year as MM_YYYY\n",
    "                formatted_date = publication_date.strftime(\"%m_%Y\")\n",
    "                \n",
    "                if link:\n",
    "                    cases_year.append((link, formatted_date))\n",
    "\n",
    "        print(f\"Case URLs of page {page_number} fetched...\")\n",
    "        page_number += 1  # Move to the next page\n",
    "\n",
    "        # if page_number > 2:\n",
    "        #     break # for debugging\n",
    "    return cases_year\n",
    "    \n",
    "# Function to extract answer_idx and answer from the clinical case page\n",
    "def extract_answers(case_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(case_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None, None  # Return None if there's an error\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Locate the section for the diagnosis\n",
    "    diagnosis_section = soup.find(\"div\", class_=\"h4 cb section-type-section\")\n",
    "    \n",
    "    # Extract the diagnosis title\n",
    "    diagnosis_title = diagnosis_section.find(\"p\", class_=\"para\").text.strip() if diagnosis_section else None\n",
    "    \n",
    "    # Now, find all the answers following the diagnosis section\n",
    "    answers = []\n",
    "    for answer in soup.find_all(\"p\", class_=\"para\"):\n",
    "        answer_text = answer.text.strip()\n",
    "        if answer_text.startswith(\"A.\") or answer_text.startswith(\"B.\") or answer_text.startswith(\"C.\") or answer_text.startswith(\"D.\"):\n",
    "            answers.append(answer_text)\n",
    "\n",
    "    # Assuming the first answer is the answer you want\n",
    "    answer_idx = answers[0][0] if answers else None  # Get the first character (A, B, C, or D)\n",
    "    answer = answers[0] if answers else None  # Get the full answer text\n",
    "    answer = answer[3:] # convert 'D. Sternoclavicular sinus' to 'Sternoclavicular sinus'\n",
    "    # pdb.set_trace()\n",
    "    return answer_idx, answer\n",
    "\n",
    "# Function to save the cases to a JSON file\n",
    "def save_to_json(cases, filename='jama_clinical_cases.json'):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(cases, json_file, indent=4)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    clinical_cases_links = scrape_clinical_cases()\n",
    "    \n",
    "    compiled_results = []\n",
    "    \n",
    "    for idx, (link, year) in enumerate(clinical_cases_links):\n",
    "        answer_idx, answer = extract_answers(link)\n",
    "        compiled_results.append({\n",
    "            'id': idx,\n",
    "            'link': link,\n",
    "            'publication_year': year,\n",
    "            'answer_idx': answer_idx,\n",
    "            'answer': answer\n",
    "        })\n",
    "    \n",
    "    # Save the results to a JSON file\n",
    "    with open('jama_clinical_cases.json', 'w') as json_file:\n",
    "        json.dump(compiled_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Saved {len(compiled_results)} cases to jama_clinical_cases.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SearchEngine",
   "language": "python",
   "name": "searchengine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
